HW2 - P8105
================
2025-09-24

##### Author: Devon Park

##### UNI: DAP2189

## Problem 1

Data to be used: `pols-month.csv`, `unemployment.csv`, and `snp.csv`
from the FiveThirtyEight data.

Goal: Merge these into a single data frame using year and month as keys
across datasets.

### Step 1.1: Import and Clean `pols-month.csv`

- Import and view data
- Break up `mon` into integer variables year, month, and day
- Replace month number with month name
- Create a president variable
- Remove variables: `prez_dem`, `prez_gop`, `day`

``` r
#Import and view data
pols_month_df =
  read_csv("fivethirtyeight_datasets/pols-month.csv") |> 
  janitor::clean_names() 
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#Create pols dataframe
pols_month_df =  
 separate(pols_month_df, mon, c("year","month","day"),"-", convert = TRUE) |>     
  mutate(
    month = month.name[month],
    president = case_when(
      prez_dem == 1 ~ "dem",
      prez_gop == 1 ~ "gop"
    )) |> 
  select(-prez_dem, -prez_gop, -day)


#When imported, everything is already <dbl> meaning it is a double numeric variable. (aka we don't need to include a line like --> na = c("NA", ".", "") that converts missing values)

#separate() --> break up [mon] into three variables: year, month, day
#year was already integer. Would need to use add the argument << year  = as.integer(year), >> to force the change

#Instead of mutate(month = month.name[month]), you could also use mutate(month = case_when(month == 1 ~ "January",....

#case_when only works here because prez_De and prez_gop values are mutually exclusive. Could alternatively have done prez_dem == 1 ~ "dem", and prez_dem == 1 ~ "gop"

#select() removes the three columns I no longer want in my dataframe
```

### Step 1.2: View and Clean `snp.csv`

- Import and view data
- Use a similar process as done with `pols-month.csv`.
- Break up `date` into integer variables year, month, and day
- Replace month number with month name
- For consistency across datasets, arrange according to year and month,
- Organize so that year and month are the leading columns

``` r
#Version 1: Slimmer 
snp_df =
  read_csv("fivethirtyeight_datasets/snp.csv") |> 
  janitor::clean_names() |> 
  separate(date, c("month","day","year"),"/", convert = TRUE) |> 
  mutate(
    year = if_else(year <= 15, year + 2000L, year + 1900L),
    month = factor(month.name[month], levels = month.name, ordered = TRUE)) |> 
  select(-day) |> 
  relocate(year) |> 
  arrange(year, month)
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#factor()--> column = month (but converted to month name), 
    #levels = month.name --> fixes the level order to month.name 
        #If i didn't specify levels =, then it would automatically be alphabetical (not by calendar month)
    #ordered = TRUE --> makes it a ordinal factor (aka, January > Febraury > March ....)
    #I could have done month = month.name[month] and then in factor(), data = month, but this seems bulkier

#put L next to 2000 and 1900 to make it an integer

#relocate() --> move year to the front of the dataframe

#arrange() sorts rows by year then by month (both in ascending order)
```

### Step 1.3: Tidy `unemployment.csv`

- Tidy the unemployment data so that it can be merged with the previous
  datasets
- switching from “wide” to “long” format
- Ensure that key variables have the same name and that key variables
  take the same values

``` r
#Import, view, pivot, and manipulate data
unemployment_df =
  read_csv("fivethirtyeight_datasets/unemployment.csv") |> 
  janitor::clean_names()|>   
  pivot_longer( 
    cols = jan:dec,
    names_to = "month",
    values_to = "unemployment_rate") |> 
  mutate(year  = as.integer(year),
        month = tolower(month),
        month = case_when( 
                            month == "jan" ~ "January",
                            month == "feb" ~ "February",
                            month == "mar" ~ "March",
                            month == "apr" ~ "April",
                            month == "may" ~ "May",
                            month == "jun" ~ "June",
                            month == "jul" ~ "July",
                            month == "aug" ~ "August",
                            month == "sep" ~ "September",
                            month == "oct" ~ "October",
                            month == "nov" ~ "November",
                            month == "dec" ~ "December"))
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

### Step 1.4: Join Data Sets

- Merge `snp.csv` INTO `pols-month.csv`,
- Merge `unemployment.csv` into above result.

``` r
snps_pols_df = 
  left_join(pols_month_df, snp_df, by = c("year","month"))
#left join: pols_month_df is the base df into which snp_df is merged. Output: new df called "snps_pols_df"


#Merged Dataframe of all three datasets
unemployment_snps_pols_df = 
  left_join(snps_pols_df, unemployment_df, by = c("year","month"))
```

### Step 1.5: Analyse Results

#### Clean Table outputs

##### `pols_month_df`

| year | month    | gov_gop | sen_gop | rep_gop | gov_dem | sen_dem | rep_dem | president |
|-----:|:---------|--------:|--------:|--------:|--------:|--------:|--------:|:----------|
| 1947 | January  |      23 |      51 |     253 |      23 |      45 |     198 | dem       |
| 1947 | February |      23 |      51 |     253 |      23 |      45 |     198 | dem       |
| 1947 | March    |      23 |      51 |     253 |      23 |      45 |     198 | dem       |
| 1947 | April    |      23 |      51 |     253 |      23 |      45 |     198 | dem       |
| 1947 | May      |      23 |      51 |     253 |      23 |      45 |     198 | dem       |

`pols_month_df` had the dimensions 822 (observations) x 9 (columns /
variables). And it contained the following variables: year, month,
gov_gop, sen_gop, rep_gop, gov_dem, sen_dem, rep_dem, president.

##### `snp_df`

| year | month    | close |
|-----:|:---------|------:|
| 1950 | January  | 17.05 |
| 1950 | February | 17.22 |
| 1950 | March    | 17.29 |
| 1950 | April    | 17.96 |
| 1950 | May      | 18.78 |

`snp_df` had the dimensions 787 (observations) x 3 (columns /
variables). And it contained the following variables: year, month,
close.

##### `unemployment_df`

| year | month    | unemployment_rate |
|-----:|:---------|------------------:|
| 1948 | January  |               3.4 |
| 1948 | February |               3.8 |
| 1948 | March    |               4.0 |
| 1948 | April    |               3.9 |
| 1948 | May      |               3.5 |

`unemployment_df` had the dimensions 816 (observations) x 3 (columns /
variables). And it contained the following variables: year, month,
unemployment_rate.

##### The complete merged dataframe: `unemployment_snps_pols_df`

| year | month | gov_gop | sen_gop | rep_gop | gov_dem | sen_dem | rep_dem | president | close | unemployment_rate |
|---:|:---|---:|---:|---:|---:|---:|---:|:---|---:|---:|
| 1947 | January | 23 | 51 | 253 | 23 | 45 | 198 | dem | NA | NA |
| 1947 | February | 23 | 51 | 253 | 23 | 45 | 198 | dem | NA | NA |
| 1947 | March | 23 | 51 | 253 | 23 | 45 | 198 | dem | NA | NA |
| 1947 | April | 23 | 51 | 253 | 23 | 45 | 198 | dem | NA | NA |
| 1947 | May | 23 | 51 | 253 | 23 | 45 | 198 | dem | NA | NA |

`unemployment_snps_pols_df` is the merged data set, containing all three
of the above datasets. It has the dimensions 822 (observations) x 11
(columns / variables). The earliest year in the dataset is 1947 and the
most recent year is 2015. However, the earliest year with unemployment
data is: 1948.

#### Additional Information & Side Notes:

Note: we could have used a date variable as a key instead of creating
year and month keys; doing so would help with some kinds of plotting,
and be a more accurate representation of the data. Date formats are
tricky, though. For more information check out the lubridate package in
the tidyverse.

------------------------------------------------------------------------

## Problem 2

This problem uses the Mr. Trash Wheel dataset, available as an Excel
file on the course website.

### Step 2.1: Read and clean the `Mr. Trash Wheel sheet`:

- Remove non-data entries and rows that do not include dumpster-specific
  data)
- Use reasonable variable names
- Round the number of sports balls to the nearest integer and converts
  the result to an integer variable

``` r
mr_trash_df =
  read_excel("Trashwheel_Collection_Data_2025.xlsx",  sheet = "Mr. Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  select(-starts_with("x"), -homes_powered) |> 
  filter(!is.na(dumpster)) |> 
  mutate(sports_balls = as.integer(round(sports_balls)),
         sheet = "mr_trash",
         year = as.double(year))
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
#skip first row in excel import (has an image)
#select(-starts_with"x") removes the two columns that R adds when importing excel data that has comments --> removes new columns ('...15' and '...16')
#removes homes_powered column because it is not dumpster specific data
#filter(!is.na(dumpster)) removes the last two rows which includes a blank row and the summary total row
#if i just used as.integer w/o round() it would just trunkate sports_balls (7.9 -> 7) rather than round (7.9 -> 8)
#create a column called sheet to prep when I merge datasets
#change variable type of year from <char> to <dbl> to match type of other two datasets (for when merging occurs)
```

### Step 2.2: Import, Clean, and Merge

- Use a similar process to import, clean, and organize the data for two
  additional sheets: Professor Trash Wheel and Gwynns Falls Trash
  Wheel  
- Combine all three to produce a single tidy dataset.

``` r
#Create df using "Professor Trash Wheel sheet (use similar methods to mr_trash_df)
prof_trash_df =
  read_excel("Trashwheel_Collection_Data_2025.xlsx",  sheet = "Professor Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  select(-homes_powered) |> 
  filter(!is.na(dumpster)) |> 
  mutate(sheet = "prof_trash")

#Create df using "Gwynns Falls Trash Wheel" sheet (use similar methods to mr_trash_df)
gwyn_trash_df =
  read_excel("Trashwheel_Collection_Data_2025.xlsx",  sheet = "Gwynns Falls Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  select(-homes_powered) |> 
  filter(!is.na(dumpster)) |> 
  mutate(sheet = "gwyn_trash")

#Combine all three datasets 
all_trash_df = 
  bind_rows(mr_trash_df, prof_trash_df, gwyn_trash_df)
```

### Step 2.3: Discuss Findings

##### Merged data: `all_trash_df`

| dumpster | month | year | date | weight_tons | volume_cubic_yards | plastic_bottles | polystyrene | cigarette_butts | glass_bottles | plastic_bags | wrappers | sports_balls | sheet |
|---:|:---|---:|:---|---:|---:|---:|---:|---:|---:|---:|---:|---:|:---|
| 1 | May | 2014 | 2014-05-16 | 4.31 | 18 | 1450 | 1820 | 126000 | 72 | 584 | 1162 | 7 | mr_trash |
| 2 | May | 2014 | 2014-05-16 | 2.74 | 13 | 1120 | 1030 | 91000 | 42 | 496 | 874 | 5 | mr_trash |
| 3 | May | 2014 | 2014-05-16 | 3.45 | 15 | 2450 | 3100 | 105000 | 50 | 1080 | 2032 | 6 | mr_trash |
| 4 | May | 2014 | 2014-05-17 | 3.10 | 15 | 2380 | 2730 | 100000 | 52 | 896 | 1971 | 6 | mr_trash |
| 5 | May | 2014 | 2014-05-17 | 4.06 | 18 | 980 | 870 | 120000 | 72 | 368 | 753 | 7 | mr_trash |

These data come from three different datasets:
`gwyn_trash_df`,`prof_trash_df`, `mr_trash_df`. Combined into one
dataset called `all_trash_df`, which has 1188 observations and 14
columns with the following variables: dumpster, month, year, date,
weight_tons, volume_cubic_yards, plastic_bottles, polystyrene,
cigarette_butts, glass_bottles, plastic_bags, wrappers, sports_balls,
sheet.

The total weight of trash collected by Professor Trash Wheel is `282.26`
tons based on this dataset.

The Gwynns Fall Trash collector, formerly know as Gwynnda, collected a
total of `18,120` cigarette butts in June, 2022.

------------------------------------------------------------------------

## Problem 3

In this question, we look at the Zillow Observed Rent Index (ZORI) in
New York City between January 2015 and August 2024.

NYC is divided into five boroughs. Each of these boroughs is it’s own
county, and in some cases the borough name and county name differ; for
example, Manhattan is New York County. Moreover, boroughs are divided
into neighborhoods. Rental price data provided by Zillow does not
include information neighborhoods within boroughs, but can be accessed
separately.

### Step 3.1: Create a single dataset

- Create a single, well-organized dataset with all the information
  contained in these data files.

``` r
#Import, clean, tidy, and otherwise wrangle each of these datasets
zipcodes_df =
  read_csv("zillow_data/Zip Codes.csv") |> 
  janitor::clean_names() |> 
  mutate(county = paste0(county, " County")) |> 
  relocate(county, zip_code)
```

    ## Rows: 322 Columns: 7
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (4): County, County Code, File Date, Neighborhood
    ## dbl (3): State FIPS, County FIPS, ZipCode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#mutate(county = paste0(county, " County")) --> paste0 adds "County" to the end of each value in [County]

nyc_zori_df = 
  read_csv("zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |> 
   pivot_longer(
    cols = starts_with("20"),
    names_to = "date",
    values_to = "zori") |> 
    janitor::clean_names() |> 
    rename(zip_code = region_name,
           county_zori = county_name) |> 
    select(-state_name, -region_type) |> 
    relocate(county_zori,zip_code,zori, size_rank) 
```

    ## Rows: 149 Columns: 125
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr   (6): RegionType, StateName, State, City, Metro, CountyName
    ## dbl (119): RegionID, SizeRank, RegionName, 2015-01-31, 2015-02-28, 2015-03-3...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#Merge datasets
zip_nyc_zori_merged_df = 
  left_join(nyc_zori_df, zipcodes_df, by = "zip_code", relationship = "many-to-many") |> 
  relocate(zip_code,county, county_zori)
```

``` r
#Exploring why when I merge the dataframes, I get two unique county columns  

#see what values are in my county column in each df --> verifying that I am working with the same possible entries in county
counties_zip =
  zipcodes_df |>  distinct(county)

counties_zori =
  nyc_zori_df |>  distinct(county_zori)


#Check if there are any rows where in the merged dataset, value of [county] does not equal value in [county_zori]
counties_unmatched_in_merged_df =
  zip_nyc_zori_merged_df |> 
  filter(county != county_zori)
#output here is 348 observations meaning that 348 rows have non-matching county information



#Checking if we had any duplicates -->4 values with duplicates --> aka 2 distinct
any(duplicated(zipcodes_df$zip_code))
```

    ## [1] TRUE

``` r
zipcodes_df |> 
  filter(duplicated(zip_code) | duplicated(zip_code, fromLast = TRUE))
```

    ## # A tibble: 4 × 7
    ##   county      zip_code state_fips county_code county_fips file_date neighborhood
    ##   <chr>          <dbl>      <dbl> <chr>             <dbl> <chr>     <chr>       
    ## 1 Bronx Coun…    10463         36 005               36005 7/25/07   Kingsbridge…
    ## 2 Kings Coun…    11201         36 047               36047 7/25/07   Northwest B…
    ## 3 New York C…    10463         36 061               36061 7/25/07   Kingsbridge…
    ## 4 New York C…    11201         36 061               36061 7/25/07   Northwest B…

Note: After merging `nyc_zori_df` and `zipcodes_df` I would ahve
expected the number of observations in my new merged data set,
`zip_nyc_zori_merged_df`, to equal the number of observations in
`nyc_zori_df`, 17284 because of how I joined them. However, the number
of obervations in the merged dataset is greater than expected: 17516. To
understand why this is the case, I looked at how the county variable is
interacting (because it did not merge these columns which is what was
expected if they were all the same values).\* After comparing county
values in both datasets, I found that there are 348 observations where
the county from the original zori dataset does not match up with the
county from the zipcodes dataset.

##### `counties_unmatched_in_merged_df`

| zip_code | county | county_zori | zori | size_rank | region_id | state | city | metro | date | state_fips | county_code | county_fips | file_date | neighborhood |
|---:|:---|:---|---:|---:|---:|:---|:---|:---|:---|---:|:---|---:|:---|:---|
| 10463 | New York County | Bronx County | NA | 183 | 61803 | NY | New York | New York-Newark-Jersey City, NY-NJ-PA | 2015-01-31 | 36 | 061 | 36061 | 7/25/07 | Kingsbridge and Riverdale |
| 10463 | New York County | Bronx County | NA | 183 | 61803 | NY | New York | New York-Newark-Jersey City, NY-NJ-PA | 2015-02-28 | 36 | 061 | 36061 | 7/25/07 | Kingsbridge and Riverdale |
| 10463 | New York County | Bronx County | NA | 183 | 61803 | NY | New York | New York-Newark-Jersey City, NY-NJ-PA | 2015-03-31 | 36 | 061 | 36061 | 7/25/07 | Kingsbridge and Riverdale |
| 10463 | New York County | Bronx County | NA | 183 | 61803 | NY | New York | New York-Newark-Jersey City, NY-NJ-PA | 2015-04-30 | 36 | 061 | 36061 | 7/25/07 | Kingsbridge and Riverdale |
| 10463 | New York County | Bronx County | 1832.289 | 183 | 61803 | NY | New York | New York-Newark-Jersey City, NY-NJ-PA | 2015-05-31 | 36 | 061 | 36061 | 7/25/07 | Kingsbridge and Riverdale |

\*Note: I later changed the names of the county columns to identify
which county column is coming from which dataset. `county_zori` is the
county column from `nyc_zori_df` and `county` is the column from
`zipcodes_df`. Otherwise, R automatically differentiates them by saying
`county.x` and `county.y`, but when R does this, I dont know which one
comes from which dataframe.

### Step 3.2: Describe Results

- Briefly describe the resulting tidy dataset (merged of zipcodes and
  zori data)

##### Merged Dataset: `zip_nyc_zori_merged_df`

| zip_code | county | county_zori | zori | size_rank | region_id | state | city | metro | date | state_fips | county_code | county_fips | file_date | neighborhood |
|---:|:---|:---|---:|---:|---:|:---|:---|:---|:---|---:|:---|---:|:---|:---|
| 11368 | Queens County | Queens County | NA | 4 | 62080 | NY | New York | New York-Newark-Jersey City, NY-NJ-PA | 2015-01-31 | 36 | 081 | 36081 | 7/25/07 | West Queens |
| 11368 | Queens County | Queens County | NA | 4 | 62080 | NY | New York | New York-Newark-Jersey City, NY-NJ-PA | 2015-02-28 | 36 | 081 | 36081 | 7/25/07 | West Queens |
| 11368 | Queens County | Queens County | NA | 4 | 62080 | NY | New York | New York-Newark-Jersey City, NY-NJ-PA | 2015-03-31 | 36 | 081 | 36081 | 7/25/07 | West Queens |
| 11368 | Queens County | Queens County | NA | 4 | 62080 | NY | New York | New York-Newark-Jersey City, NY-NJ-PA | 2015-04-30 | 36 | 081 | 36081 | 7/25/07 | West Queens |
| 11368 | Queens County | Queens County | NA | 4 | 62080 | NY | New York | New York-Newark-Jersey City, NY-NJ-PA | 2015-05-31 | 36 | 081 | 36081 | 7/25/07 | West Queens |

In `zip_nyc_zori_merged_df` there are 17516 total observations. Within
this merged dataframe, there are 149 unique zipcodes and 42 unique
neighborhoods.

### Step 3.3: Zipcodes and Graphics

- Which ZIP codes appear in the ZIP code dataset but not in the Zillow
  Rental Price dataset? Using a few illustrative examples discuss why
  these ZIP codes might be excluded from the Zillow dataset.

``` r
#Find ZIP codes in zipcodes_df but NOT in nyc_zori_df
zip_only_df = 
  anti_join(zipcodes_df, nyc_zori_df, by = "zip_code")


#Find unique neighborhoods 
distinct_neighborhoods_df =
  zip_only_df |> 
  distinct(neighborhood)|> 
  filter(!is.na(neighborhood)) |> 
  arrange(neighborhood)


#calculate how any observations have NA in the neighborhood column 
na_neighborhoods_df =
  zip_only_df |> 
  filter(is.na(neighborhood))


#Find 10 randomly generated zip codes from the non-zillow df to research what type of zipcodes are not being included:
set.seed(32)  # for reproducibility, optional
sampled_zipcodes =
  sample_n(zip_only_df, 10)
```

`zip_only_df` includes zip codes that do not appear in `nyc_zori_df`,
but instead only appear in dataframe `zipcodes_df`. This represents
zipcodes that Zillow is not tracking. There are 171 zip codes in this
dataset. Additionally, there are 14 unique neighborhoods in this
dataset. However, it should be noted that there are 137 observations in
the dataset with `NA` as the value in the neighborhood column.

I randomly created a list of 10 zipcodes from my dataframe,
`zip_only_df`. After researching these, as well as a few others, the zip
codes that are not in the zillow dataset seem to be mostly for
non-residential areas in NY (near JFK, Moynihan Station, Citi Field,
Wall St). For zip codes in this data set (not being tracked by zillow)
that are for possible residential areas, they seem to have limited
residential property listings on Zillow. Additionally some of zipcodes
are for po boxes.

### Step 3.4: Covid Fluctuations

Rental prices fluctuated dramatically during the COVID-19 pandemic. For
all available ZIP codes, compare rental prices in January 2021 to prices
in January 2020. Make a table that shows the 10 ZIP codes (along with
the borough and neighborhood) with largest drop in price from January
2020 to 2021. Comment.

``` r
#Filter for January 2020 and January 2021
jan_prices =
  zip_nyc_zori_merged_df |> 
  filter(date %in% c("2020-01-31", "2021-01-31")) |> 
  select(zip_code, county, neighborhood, date, zori)


#Reshape table to Wide Format
jan_prices_wide = 
  jan_prices |> 
  pivot_wider(
    names_from = date,
    values_from = zori,
    names_prefix = "zori_") |> 
  janitor::clean_names()


#Compute Price Change and Arrange by Largest Drop
#I could do this in one step, but for illustrative purposes, I made it two.
jan_prices_wide = 
  jan_prices_wide |> 
  mutate(
    price_drop = round(zori_2021_01_31 - zori_2020_01_31, 2)) |>
  arrange(price_drop)  


#Select Top 10 ZIP Codes with Largest Drop
top10_drop = 
  jan_prices_wide |> 
  head(n = 10) |> 
  select(zip_code, county, neighborhood, zori_2021_01_31, zori_2020_01_31, price_drop) |> 
  relocate(zip_code,price_drop) |> 
  mutate(zori_2021_01_31 = round(zori_2021_01_31, 2),
         zori_2020_01_31 = round(zori_2020_01_31, 2)) |> 
  rename("price_2020" = "zori_2020_01_31",
         "price_2021" = "zori_2021_01_31")


#Greatest price drop (reduces df to 1 row)
biggest_drop = 
  top10_drop |> 
  slice_min(price_drop, with_ties = TRUE)

#Use filter(date >= "2020-01-31", date <= "2021-01-31") if you want the range of dates between these rather than two specific dates
#arrange(price_drop) -->  Most negative (largest drop) at the top
#shorter way --> mutate(across(c(zori_2021_01_31, zori_2020_01_31), ~ round(.x, 2)))
#slice_min() --> returns the rows with the smallest values (aka the greatest price drop)
#with_ties = TRUE --> (true is the default value) If multiple rows share the cutoff value, return all of them
```

##### Zipocdes with Greatest Price Drop between 2020 and 2021: `top10_drop`

| zip_code | price_drop | county | neighborhood | price_2021 | price_2020 |
|---:|---:|:---|:---|---:|---:|
| 10007 | -912.60 | New York County | Lower Manhattan | 5421.61 | 6334.21 |
| 10069 | -748.12 | New York County | NA | 3874.92 | 4623.04 |
| 10009 | -714.25 | New York County | Lower East Side | 2692.19 | 3406.44 |
| 10016 | -711.70 | New York County | Gramercy Park and Murray Hill | 3019.43 | 3731.14 |
| 10001 | -710.45 | New York County | Chelsea and Clinton | 3397.65 | 4108.10 |
| 10002 | -710.30 | New York County | Lower East Side | 2935.11 | 3645.42 |
| 10004 | -705.96 | New York County | Lower Manhattan | 2443.70 | 3149.66 |
| 10038 | -697.59 | New York County | Lower Manhattan | 2875.62 | 3573.20 |
| 10012 | -686.22 | New York County | Greenwich Village and Soho | 2942.34 | 3628.57 |
| 10010 | -684.93 | New York County | Gramercy Park and Murray Hill | 3012.35 | 3697.28 |

Rental prices fluctuated dramatically during the COVID-19 pandemic. For
all available ZIP codes, compare rental prices in January 2021 to prices
in January 2020. Make a table that shows the 10 ZIP codes (along with
the borough and neighborhood) with largest drop in price from January
2020 to 2021. Comment.

There are 1 counties represented and 5 distinct neighborhoods with 1
neighborhood value of `NA`.

The mean price drop across the top ten largest price drops is -728.21
dollars between 2020 and 2021 (with the median price drop for this same
dataset and time frame being \$ -710.38 USD)

The zip code with the greatest price drop is `10007`, and the
neighborhood with the greatest price drip is Lower Manhattan with a
price drop value of \$ `-912.60`.
